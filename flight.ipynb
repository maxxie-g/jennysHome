{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import requests\n",
    "import re\n",
    "import airportsdata\n",
    "import googlemaps\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is to reinstantiate the DataFrame to continue working on the process\n",
    "# of adding has_crossfit to the dataset. \n",
    "mainFrame = pd.read_csv(\"dataset.csv\",index_col=0)\n",
    "\n",
    "cities = list(mainFrame[\"city\"])\n",
    "for i in range(len(cities)):\n",
    "    cities[i] = cities[i].lower()\n",
    "    cities[i] = re.sub(r'\\s', '-', cities[i])\n",
    "\n",
    "countries = list(mainFrame[\"country\"])\n",
    "for i in range(len(countries)):\n",
    "    countries[i] = countries[i].lower()\n",
    "\n",
    "city_info = {cities[i]: countries[i] for i in range(len(cities))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all airports with direct flights to Casablanca CMN\n",
    "print(\"Making soup\")\n",
    "r = requests.get(\"https://www.flightconnections.com/flights-to-casablanca-cmn\")\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "\n",
    "print(\"Extracting airports\")\n",
    "directFlights = soup.find('div', attrs={'id':'popular-destinations'})\n",
    "listFlights = directFlights.find_all('a', attrs={'class':'popular-destination'})\n",
    "airports = []\n",
    "for result in listFlights:\n",
    "    airport = result.find('div', attrs = {'class' : 'popular-destination-airport-name'})\n",
    "    airports.append(airport.text)\n",
    "\n",
    "# Getting List of International Schools\n",
    "print(\"Getting Schools List\")\n",
    "r = requests.get(\"https://www.international-schools-database.com/in?filter=on&ages=12-18&city=&language=English\")\n",
    "soup = BeautifulSoup(r.content, 'html.parser')\n",
    "allSchools = soup.find('div', attrs = {'class' : 'categories'})\n",
    "schools = allSchools.find_all('a', attrs = {'class' : 'categories-link'})\n",
    "listSchools = []\n",
    "for school in schools:\n",
    "    listSchools.append(school.text)\n",
    "\n",
    "# Extract IATA Airport Codes from the list of cities\n",
    "print(\"Getting IATA Codes\")\n",
    "mainFrame = pd.DataFrame()\n",
    "pattern = r'\\(([A-Z]{3})\\)'\n",
    "airportsData = airportsdata.load('IATA')\n",
    "cities = []\n",
    "countries = []\n",
    "\n",
    "for airport in airports:\n",
    "    # Extract IATA Code from the results\n",
    "    match = re.search(pattern,airport)\n",
    "    iata_code = match.group(1)\n",
    "\n",
    "    # Get City and Country from IATA Code\n",
    "    try:\n",
    "        city = airportsData[iata_code][\"city\"]\n",
    "        country = airportsData[iata_code][\"country\"]\n",
    "    except:\n",
    "        KeyError\n",
    "    # Check if City is already in the list, then we check if that city has an\n",
    "    # international school. If it passes through, we add it to the mainFrame.\n",
    "    if city in cities:\n",
    "        continue\n",
    "    else:\n",
    "        for school in listSchools:\n",
    "            if city.lower() in school.lower() and city != \"\":\n",
    "                cities.append(city)\n",
    "                countries.append(country)\n",
    "                print(city + \", \" + country + \" matches.\")\n",
    "            else:\n",
    "                continue\n",
    "mainFrame[\"city\"] = cities\n",
    "mainFrame[\"country\"] = countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking city_info dict and running it through this script to obtain walkability scores, then adding them to the dataset.\n",
    "# We cannot extract transit scores as the html that gets returned does not contain transit scores, so we are limited to\n",
    "# walkability scores for this dataset.\n",
    "walkabilityScores = []\n",
    "for city, country in city_info.items():\n",
    "    query = f\"{city}-{country}\"\n",
    "    r = requests.get(f\"https://www.walkscore.com/score/{query}\")\n",
    "    soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    walkScoreImg = soup.find(\"div\", attrs={\"class\": \"block-header-badge score-info-link\"})\n",
    "    img = walkScoreImg.find(\"img\")\n",
    "    imgAlt = img.get('alt')\n",
    "    match = re.search(r'\\d+', imgAlt)\n",
    "    walkScore = int(match.group())\n",
    "    walkabilityScores.append(walkScore)\n",
    "    pprint(query)\n",
    "\n",
    "mainFrame[\"walkability_score\"] = walkabilityScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Google Places API provided by Google Maps to look for places with CrossFit\n",
    "# In order to properly add the has_crossfit column to the DataFrame, we need to do some\n",
    "# string magic to be able to use the .loc() function later on.\n",
    "\n",
    "def capitalize_last_word(input_string):\n",
    "    # Split the input string into a list of words\n",
    "    words = input_string.split()\n",
    "\n",
    "    # Check if the list has any words\n",
    "    if words:\n",
    "        # Capitalize the last word in the list\n",
    "        words[-1] = words[-1].capitalize()\n",
    "    \n",
    "    # Join the words back together to return the final string\n",
    "    return ' '.join(words)\n",
    "\n",
    "file_path = 'gmaps_key.txt'\n",
    "with open(file_path, 'r') as file:\n",
    "    key = file.read()\n",
    "gmaps = googlemaps.Client(key = key)\n",
    "\n",
    "for city, country in city_info.items():\n",
    "    city_geocode = gmaps.geocode(f\"{city}, {country}\")\n",
    "    places = gmaps.places(query = \"CrossFit\", location = city_geocode[0][\"geometry\"][\"location\"], radius = 5000)\n",
    "\n",
    "    # This is part of the aforementioned string magic necessary to use it later\n",
    "    city_title = city.capitalize()\n",
    "    city_title = re.sub(r'-', ' ', city_title)\n",
    "    city_title = capitalize_last_word(city_title)\n",
    "\n",
    "    # If the returned results are greater than 0, it is safe to assume that that city\n",
    "    # has CrossFit, so has_crossfit for that city is set to True.\n",
    "    if len(places[\"results\"]) > 0:\n",
    "        pprint(f\"{city_title}, {country} has CrossFit\")\n",
    "        mainFrame.loc[mainFrame['city'] == city_title, 'has_crossfit'] = True\n",
    "    else:\n",
    "        pprint(f\"{city_title}, {country} does not have CrossFit\")\n",
    "        mainFrame.loc[mainFrame['city'] == city_title, 'has_crossfit'] = False\n",
    "    \n",
    "    # As the Google Maps API is rate limited, we output the files to use later\n",
    "    with open(f'./crossfit/{city}-{country}.json', 'w') as fp:\n",
    "        json.dump(places, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell was used to sequentially read the output .json files for further processing\n",
    "# I am fully aware that this can all be done when we first call the gmaps.places()\n",
    "# query, but I am too lazy to keep track of rate limits and felt that this would be\n",
    "# a better option.\n",
    "\n",
    "folder_path = \"./crossfit/\"\n",
    "\n",
    "# The below function is the same as the above cell\n",
    "def capitalize_last_word(input_string):\n",
    "    words = input_string.split()\n",
    "    if words:\n",
    "        words[-1] = words[-1].capitalize()\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Check if the folder path exists\n",
    "if os.path.exists(folder_path):\n",
    "    # Loop through every file in the folder path\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Check if the file ends with .json so that only json files are processed\n",
    "        if file_name.endswith(\".json\"):\n",
    "            file_path = os.path.join(folder_path,file_name)\n",
    "\n",
    "            # Similiar string magic as above\n",
    "            temp = os.path.splitext(file_name)[0][:-3].capitalize()\n",
    "            temp2 = re.sub(r'-', ' ', temp)\n",
    "            name_from_file = capitalize_last_word(temp2)\n",
    "            \n",
    "            # Check and verify that the name extracted from the file name is in the DataFrame\n",
    "            if name_from_file in mainFrame['city'].values:\n",
    "                # Get the index value of the city\n",
    "                index_value = mainFrame.index[mainFrame['city'] == name_from_file][0]\n",
    "                print(f\"{name_from_file} in DataFrame at Index {index_value}\")\n",
    "\n",
    "                # Open the .json file and do processing\n",
    "                with open(file_path, 'r') as file:\n",
    "                    result = json.load(file)\n",
    "                \n",
    "                crossfit_gyms = []\n",
    "                # Get the names of the CrossFit gyms and put them into a list.\n",
    "                for gym in result[\"results\"]:\n",
    "                    crossfit_gyms.append(gym[\"name\"])\n",
    "                mainFrame.at[index_value, 'crossfit_gyms'] = crossfit_gyms\n",
    "\n",
    "            else:\n",
    "                print(f\"{name_from_file} not in DataFrame\")\n",
    "\n",
    "# Output the DataFrame to the dataset.csv file\n",
    "mainFrame.to_csv(\"dataset.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jennysHome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
